{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6bPUeedQZkM"
      },
      "outputs": [],
      "source": [
        "# Colab에 Mecab 설치\n",
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYLxg2rQSRxK"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas matplotlib scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amPqzxCASaQQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdBltiiaSdl5"
      },
      "outputs": [],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/steam.txt\", filename=\"ratings_total.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMrh5TOcTD9T"
      },
      "outputs": [],
      "source": [
        "total_data = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])\n",
        "print('전체 리뷰 개수 :',len(total_data)) # 전체 리뷰 개수 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5N3nTsFTG05"
      },
      "outputs": [],
      "source": [
        "total_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4HNWBrlTax7"
      },
      "outputs": [],
      "source": [
        "total_data['label'] = np.select([total_data.ratings > 0], [1], default=0)\n",
        "total_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUShnxXTTuCp"
      },
      "outputs": [],
      "source": [
        "total_data['ratings'].nunique(), total_data['reviews'].nunique(), total_data['label'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hznGpIzT4XO"
      },
      "outputs": [],
      "source": [
        "total_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\n",
        "print('총 샘플의 수 :',len(total_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zObDeYlDT7Dq"
      },
      "outputs": [],
      "source": [
        "print(total_data.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suVeoluZT_rU"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\n",
        "print('훈련용 리뷰의 개수 :', len(train_data))\n",
        "print('테스트용 리뷰의 개수 :', len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrtFoL6sUNjK"
      },
      "outputs": [],
      "source": [
        "train_data['label'].value_counts().plot(kind = 'bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_mI468TUP8L"
      },
      "outputs": [],
      "source": [
        "print(train_data.groupby('label').size().reset_index(name = 'count'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9YWtq2uUSmy"
      },
      "outputs": [],
      "source": [
        "# 한글과 공백을 제외하고 모두 제거\n",
        "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data['reviews'].replace('', np.nan, inplace=True)\n",
        "print(train_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVF28VfcUezm"
      },
      "outputs": [],
      "source": [
        "test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거\n",
        "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKzYAjXEU72T"
      },
      "outputs": [],
      "source": [
        "mecab = Mecab()\n",
        "print(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiM2-qWDU_Pu"
      },
      "outputs": [],
      "source": [
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIMsnZC7VCHq"
      },
      "outputs": [],
      "source": [
        "train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\n",
        "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPgFtjtFVEI5"
      },
      "outputs": [],
      "source": [
        "test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\n",
        "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pgIioVUVH8k"
      },
      "outputs": [],
      "source": [
        "negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
        "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN_QR7RAVK2A"
      },
      "outputs": [],
      "source": [
        "negative_word_count = Counter(negative_words)\n",
        "print(negative_word_count.most_common(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQyrCiNPVOIX"
      },
      "outputs": [],
      "source": [
        "positive_word_count = Counter(positive_words)\n",
        "print(positive_word_count.most_common(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zyh1C2sVRHR"
      },
      "outputs": [],
      "source": [
        "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
        "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
        "ax1.hist(text_len, color='red')\n",
        "ax1.set_title('Positive Reviews')\n",
        "ax1.set_xlabel('length of samples')\n",
        "ax1.set_ylabel('number of samples')\n",
        "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
        "\n",
        "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
        "ax2.hist(text_len, color='blue')\n",
        "ax2.set_title('Negative Reviews')\n",
        "fig.suptitle('Words in texts')\n",
        "ax2.set_xlabel('length of samples')\n",
        "ax2.set_ylabel('number of samples')\n",
        "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtAnTpr8VZnm"
      },
      "outputs": [],
      "source": [
        "X_train = train_data['tokenized'].values\n",
        "y_train = train_data['label'].values\n",
        "X_test= test_data['tokenized'].values\n",
        "y_test = test_data['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkFW_LN2Vb_w"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa3ja8wjVgtF"
      },
      "outputs": [],
      "source": [
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MwUSd1GVrol"
      },
      "outputs": [],
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
        "vocab_size = total_cnt - rare_cnt + 2\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibPL4hB5VufQ"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaVH23SQVwhg"
      },
      "outputs": [],
      "source": [
        "print(X_train[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHfD6tLnVyMf"
      },
      "outputs": [],
      "source": [
        "print(X_test[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR2RyrAKV3md"
      },
      "outputs": [],
      "source": [
        "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(review) for review in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YQMRwpBV8mA"
      },
      "outputs": [],
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lka8YgnFWAWA"
      },
      "outputs": [],
      "source": [
        "max_len = 80\n",
        "below_threshold_len(max_len, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f70-xLZXWDPu"
      },
      "outputs": [],
      "source": [
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMaDRR5vWLCt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZki58d7WN55"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "582uu2P5ZIKh"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "    new_sentence = mecab.morphs(new_sentence)\n",
        "    new_sentence = [word for word in new_sentence if not word in stopwords]\n",
        "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
        "    pad_new = pad_sequences(encoded, maxlen=max_len)\n",
        "\n",
        "    score = float(loaded_model.predict(pad_new))\n",
        "    if score > 0.5:\n",
        "        return \"긍정\" if score >= 0.6 else \"중립\"\n",
        "    else:\n",
        "        return \"부정\" if (1 - score) >= 0.6 else \"중립\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6FJGB-yZMLT"
      },
      "outputs": [],
      "source": [
        "sentiment_predict('서로 의지하면서 오래오래 사겼음 좋겠습니다.!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFTPPOnQZRfX"
      },
      "outputs": [],
      "source": [
        "sentiment_predict('당장 헤어져! 난 혼또니 인정못해!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ejidu2oZYKX"
      },
      "outputs": [],
      "source": [
        "sentiment_predict('이래서 아이도루한테 현질하면 안됨 ㅋㅋㅋㅋ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhUJGe0IFb82"
      },
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "comments_df = pd.read_excel(\"/content/drive/MyDrive/개별연구_최종/YouTube_Comments.xlsx\")\n",
        "\n",
        "# 댓글 데이터에 대해 감정 분석 수행\n",
        "results = []\n",
        "for sentence in comments_df['Comment']:\n",
        "    results.append(sentiment_predict(sentence))\n",
        "\n",
        "comments_df[\"Sentiment\"] = results\n",
        "\n",
        "# 새로운 파일로 저장\n",
        "comments_df.to_excel(\"/content/drive/MyDrive/개별연구_최종/Sentiment_RNN.xlsx\", index=False)\n",
        "print(\"감정 분석 결과가 'Sentiment_RNN.xlsx' 파일에 저장되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x-WjYTdW7iI"
      },
      "outputs": [],
      "source": [
        "# 비율 시각화 원그래프\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 나눔고딕 폰트 설치\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum -qq\n",
        "!fc-cache -fv\n",
        "\n",
        "# 나눔고딕 폰트 설정\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font = fm.FontProperties(fname=font_path)\n",
        "\n",
        "# matplotlib 폰트 캐시 리빌드\n",
        "def set_nanum_font():\n",
        "    font_dirs = ['/usr/share/fonts/truetype/nanum']\n",
        "    font_files = fm.findSystemFonts(fontpaths=font_dirs)\n",
        "    for font_file in font_files:\n",
        "        fm.fontManager.addfont(font_file)\n",
        "    plt.rcParams['font.family'] = 'NanumGothic'\n",
        "\n",
        "set_nanum_font()\n",
        "\n",
        "# 파일 로드\n",
        "file_path = '/content/drive/MyDrive/개별연구_최종/Sentiment_RNN.xlsx'  \n",
        "comments_df = pd.read_excel(file_path)\n",
        "\n",
        "# Sentiment 열의 값에 따라 데이터의 개수 세기\n",
        "sentiment_counts = comments_df['Sentiment'].value_counts()\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightcoral', 'lightgreen'])\n",
        "plt.title('GRU')\n",
        "plt.axis('equal')  # 원형으로 그리기\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-x5eDrMwk0p"
      },
      "outputs": [],
      "source": [
        "# 대표 댓글 추출 시각화\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 파일 경로\n",
        "file_path = '/content/drive/MyDrive/개별연구_최종/Sentiment_RNN.xlsx'\n",
        "\n",
        "# 데이터 불러오기\n",
        "comments_df = pd.read_excel(file_path)\n",
        "\n",
        "# 긍정과 부정 감정만 필터링\n",
        "filtered_comments_df = comments_df[comments_df['Sentiment'].isin(['긍정', '부정'])]\n",
        "\n",
        "# 각 감정별로 대표 댓글 3개씩 추출\n",
        "top_comments = filtered_comments_df.groupby('Sentiment').apply(lambda x: x.nlargest(3, 'Like Count')).reset_index(drop=True)\n",
        "print(top_comments)\n",
        "\n",
        "# 시각화\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "colors = {'긍정': 'lightgreen', '부정': 'lightcoral'}\n",
        "top_comments['Color'] = top_comments['Sentiment'].map(colors)\n",
        "\n",
        "for sentiment in top_comments['Sentiment'].unique():\n",
        "    subset = top_comments[top_comments['Sentiment'] == sentiment]\n",
        "    ax.barh(subset['Comment'], subset['Like Count'], color=subset['Color'], label=sentiment)\n",
        "\n",
        "ax.set_xlabel('Like Count')\n",
        "ax.set_title('Top 3 Comments by Like Count for Positive and Negative Sentiments')\n",
        "ax.legend(title='Sentiment')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c8I4FoqxDAh"
      },
      "outputs": [],
      "source": [
        "# 상위 20 키워드 시각화\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from konlpy.tag import Mecab\n",
        "from collections import Counter\n",
        "\n",
        "# 파일 경로\n",
        "file_path = '/content/drive/MyDrive/개별연구_최종/Sentiment_RNN.xlsx'\n",
        "\n",
        "# 데이터 불러오기\n",
        "comments_df = pd.read_excel(file_path)\n",
        "\n",
        "# Mecab 형태소 분석기 로드\n",
        "mecab = Mecab()\n",
        "\n",
        "# 불용어 정의\n",
        "stopwords = ['이', '그', '저', '것', '수', '들', '중', '등', '더', '때', '그것', '아', '거기', '고', '곳', '나', '왜', '어떤', '로', '거', '게', '애', '건', '듯', '설', '일', '걸', '데', '난']\n",
        "\n",
        "# 모든 댓글을 형태소 분석하여 명사만 추출\n",
        "tokens = []\n",
        "for comment in comments_df['Comment']:\n",
        "    nouns = mecab.nouns(comment)\n",
        "    filtered_nouns = [noun for noun in nouns if noun not in stopwords]\n",
        "    tokens.extend(filtered_nouns)\n",
        "\n",
        "# 토큰 빈도수 계산\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "# 상위 20개 키워드 추출\n",
        "top_20_keywords = token_counts.most_common(20)\n",
        "print(top_20_keywords)\n",
        "\n",
        "# 상위 20개 키워드 시각화\n",
        "keywords, counts = zip(*top_20_keywords)\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(keywords, counts, color='skyblue')\n",
        "plt.xlabel('Frequency')\n",
        "plt.title('Top 20 Keywords')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
